executor是怎么使用的

---

place = fluid.core.CPUPlace()

exe = fluid.Executor(place)

---

推荐使用fluid.io.Dataloader的方式，先定义一个这样的Dataloader对象，然后通过Dataloader对象的set方法，设置数据源。

因为是异步的，所以推荐。一般在工业训练时使用，但在调试的时候一般还是使用exe.run(feed=...)的方式

---

除此之外还有double_buffer_reader这个函数进一步提高读取性能，会完成从CPU到GPU的转换。

```python
data_loader = fluid.io.DataLoader.from_generator(
    feed_list=[image, label], 
    capacity=64, 
    use_double_buffer=True, 
    iterable=True
)
```

---

https://aistudio.baidu.com/aistudio/projectdetail/552176

skip-gram的问题在哪里呢

在实际情况中，vocab_size通常很大（几十万甚至几百万）

取负采样（negative_sampling）

假设有一个中心词cc*c*和一个目标词正样本tpt_p*t**p*。在Skip-gram的理想实现里，需要最大化使用cc*c*预测tpt_p*t**p*的概率。在使用softmax学习时，需要最大化tpt_p*t**p*的预测概率，同时最小化其他词表中词的预测概率。之所以计算缓慢，是因为需要对词表中的所有词都计算一遍。然而我们还可以使用另一种方法，就是随机从词表中选择几个代表词，通过最小化这几个代表词的概率，去近似最小化整体的预测概率。比如，先指定一个中心词（如“人工”）和一个目标词正样本（如“智能”），再随机在词表中采样几个目标词负样本（如“日本”，“喝茶”等）。有了这些内容，我们的skip-gram模型就变成了一个二分类任务。对于目标词正样本，我们需要最大化它的预测概率；对于目标词负样本，我们需要最小化它的预测概率。通过这种方式，我们就可以完成计算加速。上述做法，我们称之为负采样。