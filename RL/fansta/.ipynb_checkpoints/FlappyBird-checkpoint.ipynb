{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Verify Fluid Program ... \n",
      "Your Paddle Fluid works well on SINGLE GPU or CPU.\n",
      "Your Paddle Fluid works well on MUTIPLE GPU or CPU.\n",
      "Your Paddle Fluid is installed successfully! Let's start deep Learning with Paddle Fluid now\n"
     ]
    }
   ],
   "source": [
    "import paddle.fluid\n",
    "paddle.fluid.install_check.run_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.putenv('SDL_VIDEODRIVER', 'fbcon')\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "import parl\n",
    "from parl import layers\n",
    "import paddle.fluid as fluid\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import gym\n",
    "from parl.utils import logger\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_FREQ = 5 # 训练频率，不需要每一个step都learn，攒一些新增经验后再learn，提高效率\n",
    "MEMORY_SIZE = 20000    # replay memory的大小，越大越占用内存\n",
    "MEMORY_WARMUP_SIZE = 200  # replay_memory 里需要预存一些经验数据，再开启训练\n",
    "BATCH_SIZE = 32   # 每次给agent learn的数据数量，从replay memory随机里sample一批数据出来\n",
    "LEARNING_RATE = 0.001 # 学习率\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(parl.Model):\n",
    "    def __init__(self, act_dim):\n",
    "        hid1_size = 32\n",
    "        hid2_size = 16\n",
    "        # 3层全连接网络\n",
    "        self.fc1 = layers.fc(size=hid1_size, act='relu')\n",
    "        self.fc2 = layers.fc(size=hid2_size, act='relu')\n",
    "        self.fc3 = layers.fc(size=act_dim, act=None)\n",
    "\n",
    "    def value(self, obs):\n",
    "        # 定义网络\n",
    "        # 输入state，输出所有action对应的Q，[Q(s,a1), Q(s,a2), Q(s,a3)...]\n",
    "        h1 = self.fc1(obs)\n",
    "        h2 = self.fc2(h1)\n",
    "        Q = self.fc3(h2)\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(parl.Algorithm):\n",
    "    def __init__(self, model, act_dim=None, gamma=None, lr=None):\n",
    "        \"\"\" DQN algorithm\n",
    "        \n",
    "        Args:\n",
    "            model (parl.Model): 定义Q函数的前向网络结构\n",
    "            act_dim (int): action空间的维度，即有几个action\n",
    "            gamma (float): reward的衰减因子\n",
    "            lr (float): learning rate 学习率.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.target_model = copy.deepcopy(model)\n",
    "\n",
    "        assert isinstance(act_dim, int)\n",
    "        assert isinstance(gamma, float)\n",
    "        assert isinstance(lr, float)\n",
    "        self.act_dim = act_dim\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "\n",
    "    def predict(self, obs):\n",
    "        \"\"\" 使用self.model的value网络来获取 [Q(s,a1),Q(s,a2),...]\n",
    "        \"\"\"\n",
    "        return self.model.value(obs)\n",
    "\n",
    "    def learn(self, obs, action, reward, next_obs, terminal):\n",
    "        \"\"\" 使用DQN算法更新self.model的value网络\n",
    "        \"\"\"\n",
    "        # 从target_model中获取 max Q' 的值，用于计算target_Q\n",
    "        next_pred_value = self.target_model.value(next_obs)\n",
    "        best_v = layers.reduce_max(next_pred_value, dim=1)\n",
    "        best_v.stop_gradient = True  # 阻止梯度传递\n",
    "        terminal = layers.cast(terminal, dtype='float32')\n",
    "        target = reward + (1.0 - terminal) * self.gamma * best_v\n",
    "\n",
    "        pred_value = self.model.value(obs)  # 获取Q预测值\n",
    "        # 将action转onehot向量，比如：3 => [0,0,0,1,0]\n",
    "        action_onehot = layers.one_hot(action, self.act_dim)\n",
    "        action_onehot = layers.cast(action_onehot, dtype='float32')\n",
    "        # 下面一行是逐元素相乘，拿到action对应的 Q(s,a)\n",
    "        # 比如：pred_value = [[2.3, 5.7, 1.2, 3.9, 1.4]], action_onehot = [[0,0,0,1,0]]\n",
    "        #  ==> pred_action_value = [[3.9]]\n",
    "        pred_action_value = layers.reduce_sum(\n",
    "            layers.elementwise_mul(action_onehot, pred_value), dim=1)\n",
    "\n",
    "        # 计算 Q(s,a) 与 target_Q的均方差，得到loss\n",
    "        cost = layers.square_error_cost(pred_action_value, target)\n",
    "        cost = layers.reduce_mean(cost)\n",
    "        optimizer = fluid.optimizer.Adam(learning_rate=self.lr)  # 使用Adam优化器\n",
    "        optimizer.minimize(cost)\n",
    "        return cost\n",
    "    \n",
    "    def sync_target(self):\n",
    "        \"\"\" 把 self.model 的模型参数值同步到 self.target_model\n",
    "        \"\"\"\n",
    "        self.model.sync_weights_to(self.target_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(parl.Agent):\n",
    "    def __init__(self,\n",
    "                 algorithm,\n",
    "                 obs_dim,\n",
    "                 act_dim,\n",
    "                 e_greed=0.1,\n",
    "                 e_greed_decrement=0):\n",
    "        assert isinstance(obs_dim, int)\n",
    "        assert isinstance(act_dim, int)\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        super(Agent, self).__init__(algorithm)\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.update_target_steps = 200  # 每隔200个training steps再把model的参数复制到target_model中\n",
    "\n",
    "        self.e_greed = e_greed  # 有一定概率随机选取动作，探索\n",
    "        self.e_greed_decrement = e_greed_decrement  # 随着训练逐步收敛，探索的程度慢慢降低\n",
    "\n",
    "    def build_program(self):\n",
    "        self.pred_program = fluid.Program()\n",
    "        self.learn_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(self.pred_program):  # 搭建计算图用于 预测动作，定义输入输出变量\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            self.value = self.alg.predict(obs)\n",
    "\n",
    "        with fluid.program_guard(self.learn_program):  # 搭建计算图用于 更新Q网络，定义输入输出变量\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            action = layers.data(name='act', shape=[1], dtype='int32')\n",
    "            reward = layers.data(name='reward', shape=[], dtype='float32')\n",
    "            next_obs = layers.data(\n",
    "                name='next_obs', shape=[self.obs_dim], dtype='float32')\n",
    "            terminal = layers.data(name='terminal', shape=[], dtype='bool')\n",
    "            self.cost = self.alg.learn(obs, action, reward, next_obs, terminal)\n",
    "\n",
    "    def sample(self, obs):\n",
    "        sample = np.random.rand()  # 产生0~1之间的小数\n",
    "        if sample < self.e_greed:\n",
    "            act = np.random.randint(self.act_dim)  # 探索：每个动作都有概率被选择\n",
    "        else:\n",
    "            act = self.predict(obs)  # 选择最优动作\n",
    "        self.e_greed = max(\n",
    "            0.01, self.e_greed - self.e_greed_decrement)  # 随着训练逐步收敛，探索的程度慢慢降低\n",
    "        return act\n",
    "\n",
    "    def predict(self, obs):  # 选择最优动作\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        pred_Q = self.fluid_executor.run(\n",
    "            self.pred_program,\n",
    "            feed={'obs': obs.astype('float32')},\n",
    "            fetch_list=[self.value])[0]\n",
    "        pred_Q = np.squeeze(pred_Q, axis=0)\n",
    "        act = np.argmax(pred_Q)  # 选择Q最大的下标，即对应的动作\n",
    "        return act\n",
    "\n",
    "    def learn(self, obs, act, reward, next_obs, terminal):\n",
    "        # 每隔200个training steps同步一次model和target_model的参数\n",
    "        if self.global_step % self.update_target_steps == 0:\n",
    "            self.alg.sync_target()\n",
    "        self.global_step += 1\n",
    "\n",
    "        act = np.expand_dims(act, -1)\n",
    "        feed = {\n",
    "            'obs': obs.astype('float32'),\n",
    "            'act': act.astype('int32'),\n",
    "            'reward': reward,\n",
    "            'next_obs': next_obs.astype('float32'),\n",
    "            'terminal': terminal\n",
    "        }\n",
    "        cost = self.fluid_executor.run(\n",
    "            self.learn_program, feed=feed, fetch_list=[self.cost])[0]  # 训练一次网络\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = collections.deque(maxlen=max_size)\n",
    "\n",
    "    # 增加一条经验到经验池中\n",
    "    def append(self, exp):\n",
    "        self.buffer.append(exp)\n",
    "\n",
    "    # 从经验池中选取N条经验出来\n",
    "    def sample(self, batch_size):\n",
    "        mini_batch = random.sample(self.buffer, batch_size)\n",
    "        obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = [], [], [], [], []\n",
    "\n",
    "        for experience in mini_batch:\n",
    "            s, a, r, s_p, done = experience\n",
    "            obs_batch.append(s)\n",
    "            action_batch.append(a)\n",
    "            reward_batch.append(r)\n",
    "            next_obs_batch.append(s_p)\n",
    "            done_batch.append(done)\n",
    "\n",
    "        return np.array(obs_batch).astype('float32'), \\\n",
    "            np.array(action_batch).astype('float32'), np.array(reward_batch).astype('float32'),\\\n",
    "            np.array(next_obs_batch).astype('float32'), np.array(done_batch).astype('float32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "totale = 0\n",
    "\n",
    "def run_episode(env, agent, rpm):\n",
    "    global totale\n",
    "    print(totale)\n",
    "    totale += 1\n",
    "    total_reward = 0\n",
    "    env.init()\n",
    "    env.reset_game()\n",
    "    obs = list(env.getGameState().values())\n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "        action = agent.sample(obs)  # 采样动作，所有动作都有概率被尝试到\n",
    "        reward = env.act(action)\n",
    "        next_obs = list(env.getGameState().values())\n",
    "        done = env.game_over()\n",
    "        rpm.append((obs, action, reward, next_obs, done))\n",
    "\n",
    "        # train model\n",
    "        if (len(rpm) > MEMORY_WARMUP_SIZE) and (step % LEARN_FREQ == 0):\n",
    "            (batch_obs, batch_action, batch_reward, batch_next_obs,\n",
    "             batch_done) = rpm.sample(BATCH_SIZE)\n",
    "            train_loss = agent.learn(batch_obs, batch_action, batch_reward,\n",
    "                                     batch_next_obs,\n",
    "                                     batch_done)  # s,a,r,s',done\n",
    "\n",
    "        total_reward += reward\n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# 评估 agent, 跑 5 个episode，总reward求平均\n",
    "def evaluate(env, agent):\n",
    "    eval_reward = []\n",
    "    for i in range(5):\n",
    "        env.init()\n",
    "        env.reset_game()\n",
    "        obs = list(env.getGameState().values())\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            action = agent.predict(obs)  # 预测动作，只选最优动作\n",
    "            reward= env.act(action)\n",
    "            obs = list(env.getGameState().values())\n",
    "            done = env.game_over()\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        eval_reward.append(episode_reward)\n",
    "    return np.mean(eval_reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-22 16:35:38 MainThread @machine_info.py:86]\u001b[0m nvidia-smi -L found gpu count: 1\n",
      "\u001b[32m[06-22 16:35:38 MainThread @machine_info.py:86]\u001b[0m nvidia-smi -L found gpu count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/bv/d/bak/CheckInThreePigs/RL/fansta/PyGame-Learning-Environment/ple/games/flappybird/__init__.py:354: DeprecationWarning: This function is deprecated. Please call randint(25, 192 + 1) instead\n",
      "  self.pipe_max\n",
      "/media/bv/d/bak/CheckInThreePigs/RL/fansta/PyGame-Learning-Environment/ple/games/flappybird/__init__.py:354: DeprecationWarning: This function is deprecated. Please call randint(25, 192 + 1) instead\n",
      "  self.pipe_max\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-aa9c440c194b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_episode\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 训练max_episode个回合，test部分不计算入episode数量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "game = FlappyBird()\n",
    "env = PLE(game, fps=30, display_screen=False)  # CartPole-v0: 预期最后一次评估总分 > 180（最大值是200）\n",
    "action_dim = len(env.getActionSet())  # CartPole-v0: 2\n",
    "obs_shape = len(env.getGameState())  # CartPole-v0: (4,)\n",
    "\n",
    "rpm = ReplayMemory(MEMORY_SIZE)  # DQN的经验回放池\n",
    "\n",
    "# 根据parl框架构建agent\n",
    "model = Model(act_dim=action_dim)\n",
    "algorithm = DQN(model, act_dim=action_dim, gamma=GAMMA, lr=LEARNING_RATE)\n",
    "agent = Agent(\n",
    "    algorithm,\n",
    "    obs_dim=obs_shape,\n",
    "    act_dim=action_dim,\n",
    "    e_greed=0.1,  # 有一定概率随机选取动作，探索\n",
    "    e_greed_decrement=1e-6)  # 随着训练逐步收敛，探索的程度慢慢降低\n",
    "\n",
    "# 加载模型\n",
    "# save_path = './dqn_model.ckpt'\n",
    "# agent.restore(save_path)\n",
    "\n",
    "# 先往经验池里存一些数据，避免最开始训练的时候样本丰富度不够\n",
    "while len(rpm) < MEMORY_WARMUP_SIZE:\n",
    "    run_episode(env, agent, rpm)\n",
    "\n",
    "max_episode = 20000000\n",
    "\n",
    "# 开始训练\n",
    "episode = 0\n",
    "\n",
    "ps = datetime.now()\n",
    "\n",
    "while episode < max_episode:  # 训练max_episode个回合，test部分不计算入episode数量\n",
    "    # train part\n",
    "    start = datetime.now()\n",
    "    for i in range(0, 50):\n",
    "        total_reward = run_episode(env, agent, rpm)\n",
    "        episode += 1\n",
    "    end = datetime.now()\n",
    "    # test part\n",
    "    eval_reward = evaluate(env, agent)  # render=True 查看显示效果\n",
    "    logger.info('episode:{}    time:{}    e_greed:{}   test_reward:{}'.format(\n",
    "        episode, (end-start).seconds, agent.e_greed, eval_reward))\n",
    "\n",
    "# 训练结束，保存模型\n",
    "save_path = './dqn_model.ckpt'\n",
    "agent.save(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
